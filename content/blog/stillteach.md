---
title: We should still teach coding
description: Coding skills are going to be as important as ever
date: 2025-03-31
canonical: https://dev.to/fastly/we-should-still-teach-coding-3cjh
---

___This post was originally published at [dev.to/fastly](https://dev.to/fastly/we-should-still-teach-coding-3cjh).___

Software written using generative AI is all over the web. Performance and security issues abound. Open source projects are being [overwhelmed by bot traffic](https://arstechnica.com/ai/2025/03/devs-say-ai-crawlers-dominate-traffic-forcing-blocks-on-entire-countries/). There's a lot of harm being caused, but as an educator who cares about lowering barriers to software creation, I can't ignore the [democratizing potential](https://dev.to/glitch/what-is-worth-learning-41e3) of these tools either.

In this post I’d like to explore the opportunities for learning that this paradigm shift presents – from how to make your projects more efficient and secure, to learning about the [intellectual property](https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem) and [labor exploitation](https://www.technologyreview.com/2022/04/20/1050392/ai-industry-appen-scale-data-labels/) that might have gone into training the models you used.

## Understanding incentives

One thing I have learned working in education is the value of accepting people’s motivations – having a goal that a skill will help you achieve is the most effective motivator for learning. Clearly many people are finding ways to use AI to help them achieve goals.

Motivations are determined by complex socioeconomic factors beyond our ability to reason away. Let’s instead keep the door to the good internet open by empowering people to make responsible choices, equipping them with knowledge of the technical systems they’re creating within. 

Gen AI might [help you spin up an MVP](https://techcrunch.com/2025/03/06/a-quarter-of-startups-in-ycs-current-cohort-have-codebases-that-are-almost-entirely-ai-generated/), but eventually you will have to dig into the code. You’ll either learn software engineering skills, or have to bring in the experts. Far from making engineers obsolete, I worry that less investment in these skills will make the barriers around the profession more extreme. ***A select few having insight into how critical systems work is not a future I’m enthusiastic about.*** Instead I’d like to find learning paths that these new ways of building software present.

## The conditions for learning are in place

Projects developed using gen AI can support a range of good practices in education, including some key activities that we often neglect in both formal learning and the workplace:

* **Personalization**: We can work on something meaningful to the learner rather than dragging them through a one-size-fits-all experience.  
* **Fast feedback loops**: Automation can fuel adaptive learning through short iterative cycles.
* **Reflection**: Reflecting on an experience helps you internalize skills you’ve acquired and reapply them in different contexts. 

Code generated using LLMs enables and requires these perhaps more than “traditional” ways of making software. There are also some [coding pedagogy techniques](https://teachcomputing.org/pedagogy) that gen AI projects can lend themselves to:

* **Reading before writing** – code comprehension is a crucial software engineering skill, consider an experienced engineer performing code reviews and mentoring junior teammates.
* **Starting from a functioning application** instead of a blank slate.
* **Running** an app to see what it does, then **investigating** how it did it.
* **Taking gradual ownership** of a project by making a first edit, then turning it into something new.

> ⚠️ Generative AI is not an appropriate substitute for the many interpersonal aspects of effective learning IMO. “Mentoring” powered by [biased LLM content](https://hai.stanford.edu/news/covert-racism-ai-how-language-models-are-reinforcing-outdated-stereotypes) is incredibly dangerous.

## Encouraging inquiry and independent thought

I wonder about a gen AI coding exercise that gives you a broken project to fix, or prompts you to explore where the code came from – this kind of inquiry is being used in the humanities to support the development of [critical thinking skills](https://www.cambridge.org/elt/blog/2023/03/30/enhancing-learners-critical-thinking-skills-with-ai-assisted-technology/), *especially important since these tools might by default [erode those very skills](https://www.microsoft.com/en-us/research/publication/the-impact-of-generative-ai-on-critical-thinking-self-reported-reductions-in-cognitive-effort-and-confidence-effects-from-a-survey-of-knowledge-workers/)*.

Then we have the wealth of problems that arise when people deploy applications with little to no understanding of their implementation. What happens over the long term, what happens if your project becomes a *real thing* people depend on, what happens when it breaks, what happens when it causes harm, how do we navigate the obfuscation of accountability? These are learning opportunities we’ll embrace if we want people to make more informed choices about how they use technology.

Software engineering is a very privileged profession, largely because it requires access to education. [Vibe coding](https://simonwillison.net/2025/Mar/19/vibe-coding/) creates new paths into building with tech. The starting point may be different, but we might even manage to invite more people into the spaces where we shape the future of the web.
